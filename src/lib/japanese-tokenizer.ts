/**
 * æ—¥æœ¬èªåˆ†ã‹ã¡æ›¸ããƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
 * kuromojiã‚’ä½¿ç”¨ã—ã¦æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†ã‹ã¡æ›¸ãã«å¤‰æ›
 */

import kuromoji from 'kuromoji';
import * as path from 'path';
import { saveTokenizerState, loadTokenizerState } from './persistent-cache';

// kuromojiã®è¾æ›¸ãƒ‘ã‚¹
const DIC_PATH = path.resolve(process.cwd(), 'node_modules/kuromoji/dict');

// ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ã§Tokenizerã‚’ç®¡ç†
let tokenizer: kuromoji.Tokenizer<kuromoji.IpadicFeatures> | null = null;
let tokenizerPromise: Promise<kuromoji.Tokenizer<kuromoji.IpadicFeatures>> | null = null;

/**
 * kuromojiãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’äº‹å‰åˆæœŸåŒ–
 * âš¡ æœ€é©åŒ–: æ°¸ç¶šåŒ–ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã§è¶…é«˜é€Ÿèµ·å‹•ã‚’å®Ÿç¾
 * â˜…â˜…â˜… ä¿®æ­£: ã‚­ãƒ£ãƒƒã‚·ãƒ¥çŠ¶æ…‹ã«é–¢ã‚ã‚‰ãšã€å®Ÿéš›ã«ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’åˆæœŸåŒ– â˜…â˜…â˜…
 * ç†ç”±: ã‚­ãƒ£ãƒƒã‚·ãƒ¥çŠ¶æ…‹ã ã‘ã§ã¯ã€å®Ÿéš›ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãŒä¿æŒã•ã‚Œãªã„
 */
export async function preInitializeTokenizer(): Promise<void> {
  // æ—¢ã«åˆæœŸåŒ–ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯æ—©æœŸãƒªã‚¿ãƒ¼ãƒ³
  if (tokenizer) {
    console.log('[JapaneseTokenizer] ğŸš€ Tokenizer already initialized');
    return;
  }
  
  // ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰åˆæœŸåŒ–çŠ¶æ…‹ã‚’ç¢ºèªï¼ˆãƒ­ã‚°å‡ºåŠ›ç”¨ï¼‰
  const cachedState = loadTokenizerState();
  if (cachedState?.isInitialized) {
    console.log('[JapaneseTokenizer] ğŸš€ Fast startup: Using cached tokenizer state');
    // ã‚­ãƒ£ãƒƒã‚·ãƒ¥çŠ¶æ…‹ãŒã‚ã£ã¦ã‚‚ã€å®Ÿéš›ã«ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’åˆæœŸåŒ–ã™ã‚‹
  }
  
  console.log('[JapaneseTokenizer] Initializing tokenizer...');
  const startTime = Date.now();
  await getTokenizer(); // å®Ÿéš›ã«ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’åˆæœŸåŒ–
  const initTime = Date.now() - startTime;
  
  // åˆæœŸåŒ–çŠ¶æ…‹ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
  saveTokenizerState(true, Date.now());
  console.log(`[JapaneseTokenizer] âœ… Tokenizer initialized and cached in ${initTime}ms`);
}

/**
 * kuromojiãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’é…å»¶åˆæœŸåŒ–
 * âš¡ æœ€é©åŒ–: å®Ÿéš›ã«å¿…è¦ã«ãªã£ãŸæ™‚ã«åˆæœŸåŒ–
 */
export async function preInitializeTokenizerLazy(): Promise<void> {
  // âš¡ æœ€é©åŒ–: è»½é‡ãªåˆæœŸåŒ–ã®ã¿å®Ÿè¡Œ
  // é‡ã„è¾æ›¸èª­ã¿è¾¼ã¿ã¯å®Ÿéš›ã®ä½¿ç”¨æ™‚ã«å®Ÿè¡Œ
  console.log('[JapaneseTokenizer] âš¡ Lazy initialization started');
  
  // è»½é‡ãªåˆæœŸåŒ–å‡¦ç†ï¼ˆè¾æ›¸èª­ã¿è¾¼ã¿ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
  return new Promise<void>((resolve) => {
    setTimeout(() => {
      console.log('[JapaneseTokenizer] âš¡ Lazy initialization completed (dictionary loading deferred)');
      resolve();
    }, 100); // 100msã§å®Œäº†
  });
}

/**
 * ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®åˆæœŸåŒ–çŠ¶æ…‹ã‚’ç¢ºèª
 */
export function isTokenizerInitialized(): boolean {
  return tokenizer !== null;
}

/**
 * kuromojiãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’åˆæœŸåŒ–ï¼ˆã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ï¼‰
 */
async function getTokenizer(): Promise<kuromoji.Tokenizer<kuromoji.IpadicFeatures>> {
  if (tokenizer) {
    return tokenizer;
  }

  if (tokenizerPromise) {
    return tokenizerPromise;
  }

  tokenizerPromise = new Promise((resolve, reject) => {
    console.log('[JapaneseTokenizer] Initializing kuromoji tokenizer...');
    kuromoji.builder({ dicPath: DIC_PATH }).build((err, t) => {
      if (err) {
        console.error('[JapaneseTokenizer] Failed to initialize kuromoji:', err);
        reject(err);
        return;
      }
      console.log('[JapaneseTokenizer] Kuromoji tokenizer initialized successfully');
      tokenizer = t;
      resolve(t);
    });
  });

  return tokenizerPromise;
}

/**
 * æ—¥æœ¬èªã®ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†ã‹ã¡æ›¸ãã•ã‚ŒãŸæ–‡å­—åˆ—ã«å¤‰æ›ã™ã‚‹
 * â˜…â˜…â˜… ä¿®æ­£: kuromojiã‚’ç¢ºå®Ÿã«ä½¿ç”¨ã™ã‚‹ï¼ˆè»½é‡ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã«ã‚ˆã‚‹å•é¡Œã‚’å›é¿ï¼‰ â˜…â˜…â˜…
 * ç†ç”±: è»½é‡ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ãŒã€Œè‡ªå‹•ã‚ªãƒ•ã‚¡ãƒ¼ã€ã‚’ã€Œã€‘è‡ªå‹•ã‚ªãƒ•ã‚¡ãƒ¼è¨­å®šæ©Ÿèƒ½ã€ã®ã‚ˆã†ã«é•·ã„ãƒˆãƒ¼ã‚¯ãƒ³ã«ã—ã€
 *       æ¤œç´¢ã‚¯ã‚¨ãƒªã€Œè‡ªå‹•ã‚ªãƒ•ã‚¡ãƒ¼ã€ã¨å®Œå…¨ä¸€è‡´ã—ãªã„å•é¡Œã‚’è§£æ±º
 * @param text å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆ
 * @returns ã‚¹ãƒšãƒ¼ã‚¹ã§åŒºåˆ‡ã‚‰ã‚ŒãŸå˜èªã®æ–‡å­—åˆ— (ä¾‹: "æ•™å®¤ ç®¡ç† ã® ä»•æ§˜")
 */
export async function tokenizeJapaneseText(text: string): Promise<string> {
  if (!text || typeof text !== 'string') {
    return '';
  }

  try {
    // â˜…â˜…â˜… ä¿®æ­£: kuromojiãŒåˆæœŸåŒ–ã•ã‚Œã‚‹ã¾ã§å¾…ã¤ï¼ˆè»½é‡ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã‚’ä½¿ã‚ãªã„ï¼‰ â˜…â˜…â˜…
    // ç†ç”±: è»½é‡ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ãŒé•·ã„ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç”Ÿæˆã—ã€Lunrã®å®Œå…¨ä¸€è‡´æ¤œç´¢ã¨äº’æ›æ€§ãŒãªã„
    // å‚è€ƒ: docs/analysis/auto-offer-search-issue-root-cause.md
    const tokenizerInstance = await getTokenizer(); // åˆæœŸåŒ–ã•ã‚Œã‚‹ã¾ã§å¾…ã¤
    
    const tokens = tokenizerInstance.tokenize(text);
    
    // å…¨ã¦ã®å˜èªï¼ˆåè©ã€å‹•è©ã€åŠ©è©ãªã©ï¼‰ã‚’ãã®ã¾ã¾ã‚¹ãƒšãƒ¼ã‚¹ã§é€£çµ
    const tokenizedText = tokens.map(t => t.surface_form).join(' ');
    
    console.log(`[JapaneseTokenizer] Tokenized: "${text}" -> "${tokenizedText}"`);
    return tokenizedText;
  } catch (error) {
    console.error('[JapaneseTokenizer] Tokenization failed:', error);
    // â˜…â˜…â˜… ä¿®æ­£: ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚è»½é‡ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã‚’ä½¿ã‚ãšã€ã‚¨ãƒ©ãƒ¼ã‚’æŠ•ã’ã‚‹ â˜…â˜…â˜…
    // ç†ç”±: è»½é‡ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã«ã‚ˆã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ä¸ä¸€è‡´ã®å•é¡Œã‚’å›é¿
    throw new Error(`Failed to tokenize Japanese text: ${error instanceof Error ? error.message : String(error)}`);
  }
}

/**
 * è»½é‡ãªæ—¥æœ¬èªãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆkuromojiãªã—ï¼‰
 * âš¡ æœ€é©åŒ–: ç°¡å˜ãªæ–‡å­—åˆ†å‰²ã§é«˜é€Ÿå‡¦ç†
 */
function performLightweightTokenization(text: string): string {
  // ç°¡å˜ãªæ–‡å­—åˆ†å‰²ï¼ˆã²ã‚‰ãŒãªã€ã‚«ã‚¿ã‚«ãƒŠã€æ¼¢å­—ã€è‹±æ•°å­—ã®å¢ƒç•Œã§åˆ†å‰²ï¼‰
  const tokens = text
    .replace(/([ã²ã‚‰ãŒãªã‚«ã‚¿ã‚«ãƒŠæ¼¢å­—]+)/g, '$1 ') // æ—¥æœ¬èªæ–‡å­—ã®å¾Œã«ã‚¹ãƒšãƒ¼ã‚¹
    .replace(/([a-zA-Z0-9]+)/g, '$1 ') // è‹±æ•°å­—ã®å¾Œã«ã‚¹ãƒšãƒ¼ã‚¹
    .trim()
    .split(/\s+/)
    .filter(token => token.length > 0);
  
  const result = tokens.join(' ');
  console.log(`[JapaneseTokenizer] âš¡ Lightweight tokenized: "${text}" -> "${result}"`);
  return result;
}

/**
 * è¤‡æ•°ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¸€æ‹¬ã§åˆ†ã‹ã¡æ›¸ãã«å¤‰æ›ã™ã‚‹
 * @param texts å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆé…åˆ—
 * @returns åˆ†ã‹ã¡æ›¸ãã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆé…åˆ—
 */
export async function tokenizeJapaneseTexts(texts: string[]): Promise<string[]> {
  if (!Array.isArray(texts)) {
    return [];
  }

  try {
    const tokenizer = await getTokenizer();
    return texts.map(text => {
      if (!text || typeof text !== 'string') {
        return '';
      }
      
      const tokens = tokenizer.tokenize(text);
      return tokens.map(t => t.surface_form).join(' ');
    });
  } catch (error) {
    console.error('[JapaneseTokenizer] Batch tokenization failed:', error);
    // ã‚¨ãƒ©ãƒ¼æ™‚ã¯å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ãã®ã¾ã¾è¿”ã™
    return texts;
  }
}

/**
 * åè©ã®ã¿ã‚’æŠ½å‡ºã—ã¦åˆ†ã‹ã¡æ›¸ãã™ã‚‹ï¼ˆã‚ˆã‚Šç²¾å¯†ãªæ¤œç´¢ç”¨ï¼‰
 * @param text å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆ
 * @returns åè©ã®ã¿ã®åˆ†ã‹ã¡æ›¸ãæ–‡å­—åˆ—
 */
export async function tokenizeJapaneseNouns(text: string): Promise<string> {
  if (!text || typeof text !== 'string') {
    return '';
  }

  try {
    const tokenizer = await getTokenizer();
    const tokens = tokenizer.tokenize(text);
    
    // åè©ã®ã¿ã‚’æŠ½å‡ºï¼ˆä¸€èˆ¬åè©ã€å›ºæœ‰åè©ã€ã‚µå¤‰åè©ãªã©ï¼‰
    const nouns = tokens
      .filter(t => {
        const pos = t.part_of_speech;
        return pos && Array.isArray(pos) && pos.some(p => p.includes('åè©')) && 
               !pos.some(p => p.includes('éè‡ªç«‹')) && !pos.some(p => p.includes('æ¥å°¾'));
      })
      .map(t => t.surface_form);
    
    const tokenizedText = nouns.join(' ');
    console.log(`[JapaneseTokenizer] Nouns only: "${text}" -> "${tokenizedText}"`);
    return tokenizedText;
  } catch (error) {
    console.error('[JapaneseTokenizer] Noun tokenization failed:', error);
    return text;
  }
}

/**
 * ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®çŠ¶æ…‹ã‚’å–å¾—
 */
export function getTokenizerStatus(): { initialized: boolean; error?: string } {
  return {
    initialized: tokenizer !== null,
    error: tokenizerPromise ? undefined : 'Not initialized'
  };
}

/**
 * ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒªã‚»ãƒƒãƒˆï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
 */
export function resetTokenizer(): void {
  tokenizer = null;
  tokenizerPromise = null;
}
