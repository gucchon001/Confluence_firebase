/**
 * æ—¥æœ¬èªåˆ†ã‹ã¡æ›¸ããƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
 * kuromojiã‚’ä½¿ç”¨ã—ã¦æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†ã‹ã¡æ›¸ãã«å¤‰æ›
 */

import kuromoji from 'kuromoji';
import * as path from 'path';
import { saveTokenizerState, loadTokenizerState } from './persistent-cache';

// kuromojiã®è¾æ›¸ãƒ‘ã‚¹
const DIC_PATH = path.resolve(process.cwd(), 'node_modules/kuromoji/dict');

// ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ã§Tokenizerã‚’ç®¡ç†
let tokenizer: kuromoji.Tokenizer<kuromoji.IpadicFeatures> | null = null;
let tokenizerPromise: Promise<kuromoji.Tokenizer<kuromoji.IpadicFeatures>> | null = null;

/**
 * kuromojiãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’äº‹å‰åˆæœŸåŒ–
 * âš¡ æœ€é©åŒ–: æ°¸ç¶šåŒ–ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã§è¶…é«˜é€Ÿèµ·å‹•ã‚’å®Ÿç¾
 */
export async function preInitializeTokenizer(): Promise<void> {
  // ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰åˆæœŸåŒ–çŠ¶æ…‹ã‚’ç¢ºèª
  const cachedState = loadTokenizerState();
  if (cachedState?.isInitialized) {
    console.log('[JapaneseTokenizer] ğŸš€ Fast startup: Using cached tokenizer state');
    return;
  }
  
  console.log('[JapaneseTokenizer] Initializing fresh tokenizer...');
  const startTime = Date.now();
  await getTokenizer();
  const initTime = Date.now() - startTime;
  
  // åˆæœŸåŒ–çŠ¶æ…‹ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
  saveTokenizerState(true, Date.now());
  console.log(`[JapaneseTokenizer] âœ… Tokenizer initialized and cached in ${initTime}ms`);
}

/**
 * kuromojiãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’é…å»¶åˆæœŸåŒ–
 * âš¡ æœ€é©åŒ–: å®Ÿéš›ã«å¿…è¦ã«ãªã£ãŸæ™‚ã«åˆæœŸåŒ–
 */
export async function preInitializeTokenizerLazy(): Promise<void> {
  // âš¡ æœ€é©åŒ–: è»½é‡ãªåˆæœŸåŒ–ã®ã¿å®Ÿè¡Œ
  // é‡ã„è¾æ›¸èª­ã¿è¾¼ã¿ã¯å®Ÿéš›ã®ä½¿ç”¨æ™‚ã«å®Ÿè¡Œ
  console.log('[JapaneseTokenizer] âš¡ Lazy initialization started');
  
  // è»½é‡ãªåˆæœŸåŒ–å‡¦ç†ï¼ˆè¾æ›¸èª­ã¿è¾¼ã¿ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
  return new Promise<void>((resolve) => {
    setTimeout(() => {
      console.log('[JapaneseTokenizer] âš¡ Lazy initialization completed (dictionary loading deferred)');
      resolve();
    }, 100); // 100msã§å®Œäº†
  });
}

/**
 * ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®åˆæœŸåŒ–çŠ¶æ…‹ã‚’ç¢ºèª
 */
export function isTokenizerInitialized(): boolean {
  return tokenizer !== null;
}

/**
 * kuromojiãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’åˆæœŸåŒ–ï¼ˆã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ï¼‰
 */
async function getTokenizer(): Promise<kuromoji.Tokenizer<kuromoji.IpadicFeatures>> {
  if (tokenizer) {
    return tokenizer;
  }

  if (tokenizerPromise) {
    return tokenizerPromise;
  }

  tokenizerPromise = new Promise((resolve, reject) => {
    console.log('[JapaneseTokenizer] Initializing kuromoji tokenizer...');
    kuromoji.builder({ dicPath: DIC_PATH }).build((err, t) => {
      if (err) {
        console.error('[JapaneseTokenizer] Failed to initialize kuromoji:', err);
        reject(err);
        return;
      }
      console.log('[JapaneseTokenizer] Kuromoji tokenizer initialized successfully');
      tokenizer = t;
      resolve(t);
    });
  });

  return tokenizerPromise;
}

/**
 * æ—¥æœ¬èªã®ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†ã‹ã¡æ›¸ãã•ã‚ŒãŸæ–‡å­—åˆ—ã«å¤‰æ›ã™ã‚‹
 * âš¡ æœ€é©åŒ–: ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ãªã„å ´åˆã¯è»½é‡ãªä»£æ›¿å‡¦ç†ã‚’ä½¿ç”¨
 * @param text å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆ
 * @returns ã‚¹ãƒšãƒ¼ã‚¹ã§åŒºåˆ‡ã‚‰ã‚ŒãŸå˜èªã®æ–‡å­—åˆ— (ä¾‹: "æ•™å®¤ ç®¡ç† ã® ä»•æ§˜")
 */
export async function tokenizeJapaneseText(text: string): Promise<string> {
  if (!text || typeof text !== 'string') {
    return '';
  }

  try {
    // âš¡ æœ€é©åŒ–: ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ãªã„å ´åˆã¯è»½é‡ãªå‡¦ç†ã‚’ä½¿ç”¨
    if (!tokenizer) {
      console.log('[JapaneseTokenizer] âš¡ Using lightweight tokenization (kuromoji not ready)');
      return performLightweightTokenization(text);
    }

    const tokenizerInstance = await getTokenizer();
    const tokens = tokenizerInstance.tokenize(text);
    
    // å…¨ã¦ã®å˜èªï¼ˆåè©ã€å‹•è©ã€åŠ©è©ãªã©ï¼‰ã‚’ãã®ã¾ã¾ã‚¹ãƒšãƒ¼ã‚¹ã§é€£çµ
    const tokenizedText = tokens.map(t => t.surface_form).join(' ');
    
    console.log(`[JapaneseTokenizer] Tokenized: "${text}" -> "${tokenizedText}"`);
    return tokenizedText;
  } catch (error) {
    console.error('[JapaneseTokenizer] Tokenization failed, using lightweight fallback:', error);
    // ã‚¨ãƒ©ãƒ¼æ™‚ã¯è»½é‡ãªä»£æ›¿å‡¦ç†ã‚’ä½¿ç”¨
    return performLightweightTokenization(text);
  }
}

/**
 * è»½é‡ãªæ—¥æœ¬èªãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆkuromojiãªã—ï¼‰
 * âš¡ æœ€é©åŒ–: ç°¡å˜ãªæ–‡å­—åˆ†å‰²ã§é«˜é€Ÿå‡¦ç†
 */
function performLightweightTokenization(text: string): string {
  // ç°¡å˜ãªæ–‡å­—åˆ†å‰²ï¼ˆã²ã‚‰ãŒãªã€ã‚«ã‚¿ã‚«ãƒŠã€æ¼¢å­—ã€è‹±æ•°å­—ã®å¢ƒç•Œã§åˆ†å‰²ï¼‰
  const tokens = text
    .replace(/([ã²ã‚‰ãŒãªã‚«ã‚¿ã‚«ãƒŠæ¼¢å­—]+)/g, '$1 ') // æ—¥æœ¬èªæ–‡å­—ã®å¾Œã«ã‚¹ãƒšãƒ¼ã‚¹
    .replace(/([a-zA-Z0-9]+)/g, '$1 ') // è‹±æ•°å­—ã®å¾Œã«ã‚¹ãƒšãƒ¼ã‚¹
    .trim()
    .split(/\s+/)
    .filter(token => token.length > 0);
  
  const result = tokens.join(' ');
  console.log(`[JapaneseTokenizer] âš¡ Lightweight tokenized: "${text}" -> "${result}"`);
  return result;
}

/**
 * è¤‡æ•°ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¸€æ‹¬ã§åˆ†ã‹ã¡æ›¸ãã«å¤‰æ›ã™ã‚‹
 * @param texts å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆé…åˆ—
 * @returns åˆ†ã‹ã¡æ›¸ãã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆé…åˆ—
 */
export async function tokenizeJapaneseTexts(texts: string[]): Promise<string[]> {
  if (!Array.isArray(texts)) {
    return [];
  }

  try {
    const tokenizer = await getTokenizer();
    return texts.map(text => {
      if (!text || typeof text !== 'string') {
        return '';
      }
      
      const tokens = tokenizer.tokenize(text);
      return tokens.map(t => t.surface_form).join(' ');
    });
  } catch (error) {
    console.error('[JapaneseTokenizer] Batch tokenization failed:', error);
    // ã‚¨ãƒ©ãƒ¼æ™‚ã¯å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ãã®ã¾ã¾è¿”ã™
    return texts;
  }
}

/**
 * åè©ã®ã¿ã‚’æŠ½å‡ºã—ã¦åˆ†ã‹ã¡æ›¸ãã™ã‚‹ï¼ˆã‚ˆã‚Šç²¾å¯†ãªæ¤œç´¢ç”¨ï¼‰
 * @param text å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆ
 * @returns åè©ã®ã¿ã®åˆ†ã‹ã¡æ›¸ãæ–‡å­—åˆ—
 */
export async function tokenizeJapaneseNouns(text: string): Promise<string> {
  if (!text || typeof text !== 'string') {
    return '';
  }

  try {
    const tokenizer = await getTokenizer();
    const tokens = tokenizer.tokenize(text);
    
    // åè©ã®ã¿ã‚’æŠ½å‡ºï¼ˆä¸€èˆ¬åè©ã€å›ºæœ‰åè©ã€ã‚µå¤‰åè©ãªã©ï¼‰
    const nouns = tokens
      .filter(t => {
        const pos = t.part_of_speech;
        return pos && Array.isArray(pos) && pos.some(p => p.includes('åè©')) && 
               !pos.some(p => p.includes('éè‡ªç«‹')) && !pos.some(p => p.includes('æ¥å°¾'));
      })
      .map(t => t.surface_form);
    
    const tokenizedText = nouns.join(' ');
    console.log(`[JapaneseTokenizer] Nouns only: "${text}" -> "${tokenizedText}"`);
    return tokenizedText;
  } catch (error) {
    console.error('[JapaneseTokenizer] Noun tokenization failed:', error);
    return text;
  }
}

/**
 * ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®çŠ¶æ…‹ã‚’å–å¾—
 */
export function getTokenizerStatus(): { initialized: boolean; error?: string } {
  return {
    initialized: tokenizer !== null,
    error: tokenizerPromise ? undefined : 'Not initialized'
  };
}

/**
 * ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒªã‚»ãƒƒãƒˆï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
 */
export function resetTokenizer(): void {
  tokenizer = null;
  tokenizerPromise = null;
}
