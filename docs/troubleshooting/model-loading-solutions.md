# モデル読み込み問題の解決策

## 代替アプローチ案

### アプローチ A: sentence-transformers に変更
**メリット**:
- より成熟したライブラリ
- ローカルモデル読み込みが確実に機能する
- ドキュメントが豊富

**デメリット**:
- コードの大幅な変更が必要
- 依存関係の変更

### アプローチ B: モデルを base64 エンコードして埋め込む
**メリット**:
- ファイルシステムの問題を完全に回避
- デプロイが簡単

**デメリット**:
- メモリ使用量が非常に大きい（tokenizer.json は 16.7MB）
- コードが肥大化

### アプローチ C: Cloud Storage FUSE を使用
**メリット**:
- ファイルを Cloud Storage に保存し、コンテナ内でマウント
- ファイルシステムの問題を回避

**デメリット**:
- セットアップが複雑
- 起動時間が長くなる可能性

### アプローチ D: モデルをビルド時にバンドルしない
**メリット**:
- 現在の環境変数設定で動作する可能性
- ネットワークアクセスが必要

**デメリット**:
- 起動が遅い（モデルダウンロード）
- レート制限の問題（429エラー）

### アプローチ E: ONNX.js を直接使用
**メリット**:
- より細かい制御が可能
- ファイル読み込みを完全に制御できる

**デメリット**:
- コードが複雑になる
- 開発時間が増加

## 推奨アプローチ

**現時点では、アプローチ D（ビルド時にモデルをバンドルしない）を推奨します。**

理由:
1. 最短で動作する
2. 環境変数の設定だけで動作する可能性
3. ユーザー体験への影響は一時的なもの（初回のみ遅い）
4. 後で最適化できる

実装方法:
1. `postbuild` スクリプトを削除
2. `apphosting.yaml` の環境変数設定を確認（`HF_HUB_OFFLINE=1` を削除）
3. モデルを初回起動時にダウンロードする

次に、アプローチ A または E を検討して、長期的な解決策を実装する。

## 各アプローチの実装難易度

| アプローチ | 実装難易度 | 時間見積もり | リスク |
|-----------|-----------|------------|--------|
| A: sentence-transformers | 高 | 1-2日 | 中 |
| B: base64エンコード | 中 | 4-6時間 | 高 |
| C: Cloud Storage FUSE | 高 | 1-2日 | 高 |
| D: ビルド時にバンドルしない | **低** | **1-2時間** | **低** |
| E: ONNX.js | 中 | 1日 | 中 |

## 次のステップ

1. **即座**: アプローチ D を試す
2. **短期**: アプローチ A を検討
3. **長期**: 最適なアプローチを選択

---

**作成日**: 2025-10-27  
**ステータス**: 推奨策の提案
