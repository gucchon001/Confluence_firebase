# RAG改善機能 段階的実装計画

**作成日**: 2025年1月  
**目的**: セマンティックチャンキング、メタデータ強化、要約機能改善を段階的に実装  
**アプローチ**: テスト駆動開発（TDD）、段階的リリース、品質検証

---

## 📋 実装項目一覧

1. ✅ セマンティックチャンキングの導入
2. ✅ チャンクサイズの統一
3. ✅ 構造メタデータの強化
4. ✅ セクション情報の表示
5. ✅ 画像のOCR対応
6. ✅ ハイブリッド要約の導入
7. ✅ 要約粒度の調整機能

---

## 🎯 全体アーキテクチャ

```
Phase 1: チャンキング改善（基盤整備）
  ↓
Phase 2: メタデータ強化（検索精度向上）
  ↓
Phase 3: 表示改善（UX向上）
  ↓
Phase 4: 高度な機能（拡張機能）
```

---

## Phase 1: チャンキング改善（3-4週間）

### 🎯 目標
- 固定サイズチャンキングからセマンティックチャンキングへ移行
- チャンクサイズの統一
- 検索精度の向上を検証

---

### Step 1.1: セマンティックチャンキング基盤実装（1週間）

#### 実装タスク

**1.1.1: セマンティックチャンキング関数の実装**

**ファイル**: `src/lib/semantic-chunking.ts` (新規作成)

```typescript
export interface SemanticChunkOptions {
  maxChunkSize?: number;      // デフォルト: 1800
  overlap?: number;            // デフォルト: 200
  respectSentenceBoundaries?: boolean;  // デフォルト: true
  respectParagraphBoundaries?: boolean; // デフォルト: true
}

export interface SemanticChunk extends TextChunk {
  sentenceStart: number;    // 開始文のインデックス
  sentenceEnd: number;      // 終了文のインデックス
  paragraphStart?: number;  // 開始段落のインデックス
  paragraphEnd?: number;    // 終了段落のインデックス
}

export function semanticChunkText(
  text: string,
  options: SemanticChunkOptions = {}
): SemanticChunk[]
```

**実装内容**:
1. 文の境界で分割（句点、改行を検出）
2. 段落の境界を考慮（空行、見出しマーカー）
3. オーバーラップ時に文の境界を維持
4. チャンクサイズ制限内で意味的まとまりを保持

**テスト**: `src/lib/__tests__/semantic-chunking.test.ts` (新規作成)

```typescript
describe('semanticChunkText', () => {
  it('文の境界で分割されること', () => {
    const text = 'これは最初の文です。これは2番目の文です。これは3番目の文です。';
    const chunks = semanticChunkText(text, { maxChunkSize: 50 });
    // 文の途中で分割されていないことを確認
  });
  
  it('段落の境界を考慮すること', () => {
    const text = '段落1\n\n段落2\n\n段落3';
    const chunks = semanticChunkText(text);
    // 段落の途中で分割されていないことを確認
  });
  
  it('オーバーラップ時に文の境界を維持すること', () => {
    // オーバーラップ部分が文の途中で始まっていないことを確認
  });
  
  it('長い文を適切に処理すること', () => {
    // maxChunkSizeを超える長い文でもエラーなく処理されることを確認
  });
});
```

**検証項目**:
- [ ] 文の境界で分割されること
- [ ] 段落の境界を考慮すること
- [ ] オーバーラップ時に文の境界を維持すること
- [ ] チャンクサイズが指定範囲内であること
- [ ] 空テキストを適切に処理すること

**完了条件**:
- ユニットテストが全てパス
- 既存の`chunkText`関数と互換性を保持
- パフォーマンステストで既存実装の2倍以内

---

**1.1.2: 日本語テキスト処理の最適化**

**実装内容**:
1. 日本語の句点（。、！、？）の検出
2. 英語の句点との区別
3. 改行の扱い（段落区切り vs 見出し）

**テスト**: 
- 日本語テキストでの動作確認
- 英語テキストでの動作確認
- 混合テキストでの動作確認

**完了条件**:
- 日本語テキストで適切に分割されること
- 英語テキストでも動作すること

---

**1.1.3: 既存チャンキング関数との比較テスト**

**ファイル**: `scripts/test-semantic-vs-fixed-chunking.ts` (新規作成)

**テスト内容**:
1. 同じテキストを固定サイズとセマンティックでチャンク分割
2. チャンク数、平均チャンクサイズ、分割位置を比較
3. 文の途中での分割回数をカウント

**検証項目**:
- [ ] セマンティックチャンキングで文の途中分割が減少すること
- [ ] チャンク数の増減が許容範囲内であること（±20%）
- [ ] 平均チャンクサイズが適切であること（1500-1800文字）

**完了条件**:
- テストレポートを生成
- 品質改善を定量的に確認

---

### Step 1.2: Confluence統合（3-4日）

#### 実装タスク

**1.2.1: Confluence同期サービスへの統合**

**ファイル**: `src/lib/confluence-sync-service.ts`

**変更内容**:
1. `splitPageIntoChunks`メソッドをセマンティックチャンキングに置き換え
2. 既存のメタデータ（chunkIndex, totalChunks）を維持
3. 後方互換性を保持

**実装コード**:
```typescript
private splitPageIntoChunks(page: ConfluencePage): ConfluenceChunk[] {
  const cleanContent = this.extractTextFromHtml(page.content || '');
  
  // セマンティックチャンキングを使用
  const semanticChunks = semanticChunkText(cleanContent, {
    maxChunkSize: 1800,
    overlap: 200,
    respectSentenceBoundaries: true,
    respectParagraphBoundaries: true,
  });
  
  // ConfluenceChunk形式に変換
  return semanticChunks.map((chunk, index) => ({
    pageId: parseInt(page.id),
    title: this.extractTextFromHtml(page.title || 'No Title'),
    content: chunk.text,
    chunkIndex: index,
    lastUpdated: page.lastModified || new Date().toISOString(),
    spaceKey: page.spaceKey || 'N/A',
    embedding: []
  }));
}
```

**テスト**: `src/lib/__tests__/confluence-semantic-chunking.test.ts` (新規作成)

**検証項目**:
- [ ] 既存のページがセマンティックチャンキングで分割されること
- [ ] chunkIndex、totalChunksが正しく設定されること
- [ ] HTMLから抽出したテキストが適切に処理されること
- [ ] 既存の検索APIが正常に動作すること

**完了条件**:
- ユニットテストが全てパス
- 既存のテストが全てパス（回帰テストなし）

---

**1.2.2: 本番データでの検証**

**ファイル**: `scripts/test-confluence-semantic-chunking-production.ts` (新規作成)

**テスト内容**:
1. 本番環境のConfluenceデータ（10-20ページ）を使用
2. 固定サイズとセマンティックでチャンク分割を比較
3. 検索結果の品質を比較

**検証項目**:
- [ ] チャンク分割が正常に完了すること
- [ ] 検索結果の精度が向上すること（または同等であること）
- [ ] パフォーマンスが許容範囲内であること

**完了条件**:
- テストレポートを生成
- 品質向上を確認（または同等であることを確認）

---

### Step 1.3: Jira統合（3-4日）

#### 実装タスク

**1.3.1: Jira同期サービスへの統合**

**ファイル**: `src/lib/jira-sync-service.ts`

**変更内容**:
1. `toLanceDbRecords`メソッド内の`chunkText`呼び出しを`semanticChunkText`に置き換え
2. 既存のチャンク分割条件（3000文字以上）を維持

**実装コード**:
```typescript
// チャンク分割が必要な場合
const chunks = semanticChunkText(content, {
  maxChunkSize: 1800,
  overlap: 200,
  respectSentenceBoundaries: true,
  respectParagraphBoundaries: true,
});
```

**テスト**: `src/lib/__tests__/jira-semantic-chunking.test.ts` (新規作成)

**検証項目**:
- [ ] 3000文字以上のIssueがセマンティックチャンキングで分割されること
- [ ] チャンクメタデータ（isChunked, chunkIndex, totalChunks）が正しく設定されること
- [ ] コメントや変更履歴の境界を考慮すること

**完了条件**:
- ユニットテストが全てパス
- 既存のJiraテストが全てパス

---

**1.3.2: 本番データでの検証**

**ファイル**: `scripts/test-jira-semantic-chunking-production.ts` (新規作成)

**テスト内容**:
1. 本番環境のJiraデータ（長いIssue 10-20件）を使用
2. 固定サイズとセマンティックでチャンク分割を比較
3. 検索結果の品質を比較

**完了条件**:
- テストレポートを生成
- 品質向上を確認

---

### Step 1.4: Google Drive統合（2-3日）

#### 実装タスク

**1.4.1: Google Drive同期サービスへの統合**

**ファイル**: `src/lib/google-drive-lancedb-service.ts`

**変更内容**:
1. `chunkText`を`semanticChunkText`に置き換え
2. チャンクサイズを1000から1800に統一（Step 2.1と同時実装）

**実装コード**:
```typescript
const chunks = semanticChunkText(document.content, {
  maxChunkSize: 1800,  // 1000から1800に変更
  overlap: 200,
  respectSentenceBoundaries: true,
  respectParagraphBoundaries: true,
});
```

**テスト**: `src/lib/__tests__/google-drive-semantic-chunking.test.ts` (新規作成)

**検証項目**:
- [ ] Google Driveドキュメントがセマンティックチャンキングで分割されること
- [ ] チャンクサイズが1800文字に統一されること
- [ ] マークダウン形式が適切に処理されること

**完了条件**:
- ユニットテストが全てパス

---

### Step 1.5: 統合テストと検証（3-4日）

#### 統合テスト

**1.5.1: 全データソースでの統合テスト**

**ファイル**: `src/tests/integration/semantic-chunking-integration.test.ts` (新規作成)

**テスト内容**:
1. Confluence、Jira、Google Driveの全てでセマンティックチャンキングが動作すること
2. 検索APIが正常に動作すること
3. チャンク統合処理（enrichWithAllChunks）が正常に動作すること

**検証項目**:
- [ ] 全データソースでセマンティックチャンキングが適用されること
- [ ] 検索結果の品質が向上すること（または同等であること）
- [ ] パフォーマンスが許容範囲内であること

---

**1.5.2: パフォーマンステスト**

**ファイル**: `scripts/test-semantic-chunking-performance.ts` (新規作成)

**テスト内容**:
1. 固定サイズとセマンティックの処理時間を比較
2. メモリ使用量を比較
3. 大量データ（1000ページ）でのパフォーマンス

**検証項目**:
- [ ] 処理時間が既存の2倍以内であること
- [ ] メモリ使用量が許容範囲内であること
- [ ] 大量データでもエラーなく処理されること

---

**1.5.3: 検索品質テスト**

**ファイル**: `scripts/test-semantic-chunking-search-quality.ts` (新規作成)

**テスト内容**:
1. 固定サイズとセマンティックの検索結果を比較
2. 検索精度（Precision、Recall）を測定
3. ユーザーテストケースでの検証

**検証項目**:
- [ ] 検索精度が向上すること（または同等であること）
- [ ] 重要な情報が欠落しないこと
- [ ] 検索結果がより関連性が高いこと

---

**Phase 1 完了条件**:
- [ ] 全ユニットテストがパス
- [ ] 全統合テストがパス
- [ ] パフォーマンステストが許容範囲内
- [ ] 検索品質テストで改善を確認
- [ ] コードレビュー完了
- [ ] 本番環境での動作確認（ステージング環境）

---

## Phase 2: チャンクサイズの統一（1週間）

### 🎯 目標
- Google Driveのチャンクサイズを1800文字に統一
- 全データソースで一貫性を確保

---

### Step 2.1: Google Driveチャンクサイズ修正（1日）

#### 実装タスク

**2.1.1: チャンクサイズの修正**

**ファイル**: `src/lib/google-drive-lancedb-service.ts`

**変更内容**:
```typescript
// 修正前
const chunks = chunkText(document.content, {
  maxChunkSize: 1000,  // 1000文字
  overlap: 200,
});

// 修正後
const chunks = semanticChunkText(document.content, {
  maxChunkSize: 1800,  // 1800文字に統一
  overlap: 200,
});
```

**テスト**: `src/lib/__tests__/google-drive-chunk-size.test.ts` (新規作成)

**検証項目**:
- [ ] チャンクサイズが1800文字であること
- [ ] 既存のGoogle Driveドキュメントが正常に処理されること

**完了条件**:
- ユニットテストがパス
- 既存のGoogle Driveテストがパス

---

### Step 2.2: 仕様書の更新（0.5日）

**ファイル**: `docs/02-specifications/02.04-google-drive-spec.md`

**変更内容**:
- チャンクサイズを1000文字から1800文字に更新

---

### Step 2.3: 統一性の検証（1日）

**ファイル**: `scripts/test-chunk-size-consistency.ts` (新規作成)

**テスト内容**:
1. Confluence、Jira、Google Driveの全データソースでチャンクサイズを確認
2. 平均チャンクサイズが1500-1800文字であることを確認

**検証項目**:
- [ ] 全データソースでチャンクサイズが1800文字であること
- [ ] オーバーラップが200文字であること

**完了条件**:
- テストレポートを生成
- 統一性を確認

---

**Phase 2 完了条件**:
- [ ] Google Driveのチャンクサイズが1800文字に統一
- [ ] 全データソースで一貫性を確認
- [ ] 仕様書を更新

---

## Phase 3: 構造メタデータの強化（2-3週間）

### 🎯 目標
- 見出し、セクション情報をメタデータとして記録
- 前後チャンク情報を記録
- 検索精度の向上

---

### Step 3.1: メタデータスキーマの拡張（2-3日）

#### 実装タスク

**3.1.1: LanceDBスキーマの拡張**

**ファイル**: `src/lib/lancedb-schema-extended.ts`

**変更内容**:
```typescript
export interface EnhancedLanceDBRecord extends ExtendedLanceDBRecord {
  // 構造メタデータ（新規追加）
  sectionTitle?: string;      // セクション見出し
  sectionLevel?: number;      // 見出しレベル（1-6）
  sectionPath?: string;       // セクションパス（例: "概要 > 詳細 > 実装"）
  pageNumber?: number;        // ページ番号（PDF等）
  
  // コンテキストメタデータ（新規追加）
  previousChunkId?: string;   // 前のチャンクID
  nextChunkId?: string;       // 次のチャンクID
  parentChunkId?: string;     // 親チャンクID（階層構造の場合）
}
```

**実装内容**:
1. 型定義の追加
2. 既存コードとの互換性を保持（オプショナルフィールド）

**テスト**: `src/lib/__tests__/enhanced-metadata-schema.test.ts` (新規作成)

**検証項目**:
- [ ] 新しいフィールドがオプショナルであること
- [ ] 既存のレコードと互換性があること

---

### Step 3.2: Markdown解析機能の実装（1週間）

#### 実装タスク

**3.2.1: Markdown見出し解析**

**ファイル**: `src/lib/markdown-parser.ts` (新規作成)

**実装内容**:
```typescript
export interface MarkdownStructure {
  sections: Array<{
    title: string;
    level: number;  // 1-6
    startIndex: number;
    endIndex: number;
    path: string;   // "概要 > 詳細 > 実装"
  }>;
}

export function parseMarkdownStructure(markdown: string): MarkdownStructure
```

**テスト**: `src/lib/__tests__/markdown-parser.test.ts` (新規作成)

**検証項目**:
- [ ] 見出し（# - ######）を正しく検出すること
- [ ] 見出しレベルを正しく判定すること
- [ ] セクションパスを正しく生成すること
- [ ] ネストされた見出しを適切に処理すること

---

**3.2.2: HTMLから見出し抽出（Confluence用）**

**ファイル**: `src/lib/html-heading-extractor.ts` (新規作成)

**実装内容**:
1. ConfluenceのHTMLから見出しタグ（h1-h6）を抽出
2. 見出し階層を構築
3. セクションパスを生成

**テスト**: `src/lib/__tests__/html-heading-extractor.test.ts` (新規作成)

**検証項目**:
- [ ] HTMLから見出しを正しく抽出すること
- [ ] 見出し階層を正しく構築すること

---

### Step 3.3: チャンク分割時のメタデータ付与（1週間）

#### 実装タスク

**3.3.1: セマンティックチャンキングへのメタデータ統合**

**ファイル**: `src/lib/semantic-chunking.ts`

**変更内容**:
1. Markdown構造を解析
2. チャンクが含む見出しを特定
3. セクション情報をメタデータとして付与

**実装コード**:
```typescript
export function semanticChunkTextWithMetadata(
  text: string,
  options: SemanticChunkOptions & {
    markdown?: string;  // Markdown形式のテキスト
    html?: string;      // HTML形式のテキスト
  }
): SemanticChunk[]
```

**テスト**: `src/lib/__tests__/semantic-chunking-metadata.test.ts` (新規作成)

**検証項目**:
- [ ] チャンクにセクション情報が付与されること
- [ ] 前後チャンクのIDが設定されること
- [ ] 見出し階層が正しく保持されること

---

**3.3.2: Confluence統合**

**ファイル**: `src/lib/confluence-sync-service.ts`

**変更内容**:
1. HTMLから見出しを抽出
2. チャンク分割時にセクション情報を付与
3. 前後チャンクのIDを設定

**テスト**: `src/lib/__tests__/confluence-metadata.test.ts` (新規作成)

**検証項目**:
- [ ] Confluenceページの見出しがメタデータに記録されること
- [ ] チャンク間の関係性が保持されること

---

**3.3.3: Jira統合**

**ファイル**: `src/lib/jira-sync-service.ts`

**変更内容**:
1. Markdown形式のコメントや説明から見出しを抽出
2. チャンク分割時にセクション情報を付与

**テスト**: `src/lib/__tests__/jira-metadata.test.ts` (新規作成)

---

**3.3.4: Google Drive統合**

**ファイル**: `src/lib/google-drive-lancedb-service.ts`

**変更内容**:
1. Google Docs、Markdownファイルから見出しを抽出
2. チャンク分割時にセクション情報を付与

**テスト**: `src/lib/__tests__/google-drive-metadata.test.ts` (新規作成)

---

### Step 3.4: LanceDBスキーマ更新（2-3日）

#### 実装タスク

**3.4.1: テーブルスキーマの更新**

**実装内容**:
1. 既存テーブルに新しいフィールドを追加（ALTER TABLE相当）
2. または、テーブル再構築スクリプトを作成

**ファイル**: `scripts/add-metadata-fields-to-lancedb.ts` (新規作成)

**注意事項**:
- 既存データとの互換性を保持
- データ移行スクリプトを作成

**テスト**: `scripts/test-metadata-migration.ts` (新規作成)

**検証項目**:
- [ ] 既存データが正常に移行されること
- [ ] 新しいフィールドが正しく設定されること
- [ ] 既存の検索機能が正常に動作すること

---

### Step 3.5: 統合テスト（3-4日）

**ファイル**: `src/tests/integration/metadata-enhancement.test.ts` (新規作成)

**テスト内容**:
1. 全データソースでメタデータが正しく設定されること
2. 検索時にメタデータが活用されること
3. 既存機能が正常に動作すること

**検証項目**:
- [ ] メタデータが全データソースで設定されること
- [ ] 検索精度が向上すること
- [ ] パフォーマンスが許容範囲内であること

---

**Phase 3 完了条件**:
- [ ] 全メタデータフィールドが実装されていること
- [ ] 全データソースでメタデータが設定されること
- [ ] 統合テストがパスすること
- [ ] 既存機能が正常に動作すること

---

## Phase 4: セクション情報の表示（1週間）

### 🎯 目標
- 検索結果にセクション情報を表示
- 参照元の明確化

---

### Step 4.1: 参照元表示の拡張（3-4日）

#### 実装タスク

**4.1.1: 検索結果の拡張**

**ファイル**: `src/lib/lancedb-search-client.ts`

**変更内容**:
1. 検索結果にセクション情報を含める
2. 参照元表示形式を拡張

**実装コード**:
```typescript
interface EnhancedSearchResult {
  // 既存フィールド
  id: string;
  title: string;
  url: string;
  
  // 新規追加
  sectionTitle?: string;
  sectionPath?: string;
  chunkIndex?: number;
}
```

**テスト**: `src/lib/__tests__/enhanced-search-results.test.ts` (新規作成)

---

**4.1.2: UI表示の拡張**

**ファイル**: `src/components/search-results.tsx` (または該当コンポーネント)

**変更内容**:
1. 参照元にセクション情報を表示
2. セクションパスを表示（例: "概要 > 詳細 > 実装"）

**表示例**:
```
参照元:
- [会員:アカウント情報 > ログイン認証] (Confluence)
- [CTJ-1234: 応募移管機能の開発 > 実装詳細] (Jira)
```

**テスト**: UIコンポーネントのテスト

---

### Step 4.2: 統合テスト（2-3日）

**ファイル**: `src/tests/integration/section-display.test.ts` (新規作成)

**検証項目**:
- [ ] セクション情報が正しく表示されること
- [ ] 参照元が明確になること

---

**Phase 4 完了条件**:
- [ ] セクション情報が検索結果に表示されること
- [ ] UIテストがパスすること

---

## Phase 5: 画像のOCR対応（3-4週間）

### 🎯 目標
- PDF内画像のOCR処理
- 画像ファイルのOCR処理
- 抽出テキストの検索対象化

---

### Step 5.1: OCR機能の実装（2週間）

#### 実装タスク

**5.1.1: Google Cloud Vision API統合**

**ファイル**: `src/lib/ocr-service.ts` (新規作成)

**実装内容**:
```typescript
export interface OCRResult {
  text: string;
  confidence: number;
  boundingBoxes?: Array<{
    x: number;
    y: number;
    width: number;
    height: number;
  }>;
}

export async function extractTextFromImage(
  imageBuffer: Buffer,
  options?: {
    language?: string;  // 'ja', 'en', etc.
  }
): Promise<OCRResult>
```

**テスト**: `src/lib/__tests__/ocr-service.test.ts` (新規作成)

**検証項目**:
- [ ] 画像からテキストが抽出されること
- [ ] 日本語テキストが正しく認識されること
- [ ] エラーハンドリングが適切であること

---

**5.1.2: PDF内画像の抽出**

**ファイル**: `src/lib/pdf-image-extractor.ts` (新規作成)

**実装内容**:
1. PDFから画像を抽出
2. 画像をOCR処理
3. 抽出テキストを統合

**ライブラリ**: `pdf-lib` または `pdf-parse`

**テスト**: `src/lib/__tests__/pdf-image-extractor.test.ts` (新規作成)

---

**5.1.3: Google Drive統合**

**ファイル**: `src/lib/google-drive-service.ts`

**変更内容**:
1. PDFファイル処理時に画像OCRを実行
2. 画像ファイルのOCR処理を追加

**テスト**: `src/lib/__tests__/google-drive-ocr.test.ts` (新規作成)

---

### Step 5.2: テストと検証（1-2週間）

**ファイル**: `scripts/test-ocr-quality.ts` (新規作成)

**テスト内容**:
1. 様々な画像でのOCR精度をテスト
2. 日本語、英語の混合テキストをテスト
3. 処理時間を測定

**検証項目**:
- [ ] OCR精度が許容範囲内であること（80%以上）
- [ ] 処理時間が許容範囲内であること

---

**Phase 5 完了条件**:
- [ ] OCR機能が実装されていること
- [ ] PDF内画像からテキストが抽出されること
- [ ] 画像ファイルからテキストが抽出されること
- [ ] 精度が許容範囲内であること

---

## Phase 6: ハイブリッド要約の導入（2週間）

### 🎯 目標
- 抽出型要約と生成型要約を組み合わせ
- 要約品質の向上

---

### Step 6.1: 抽出型要約の実装（1週間）

#### 実装タスク

**6.1.1: 重要文抽出機能**

**ファイル**: `src/lib/extractive-summarization.ts` (新規作成)

**実装内容**:
```typescript
export interface ExtractedSentence {
  text: string;
  score: number;
  index: number;
}

export function extractImportantSentences(
  text: string,
  options?: {
    maxSentences?: number;
    method?: 'tf-idf' | 'text-rank' | 'llm-based';
  }
): ExtractedSentence[]
```

**テスト**: `src/lib/__tests__/extractive-summarization.test.ts` (新規作成)

---

**6.1.2: ストリーミング要約への統合**

**ファイル**: `src/ai/flows/streaming-summarize-confluence-docs.ts`

**変更内容**:
1. 検索結果から重要文を抽出
2. 抽出文を生成型要約のコンテキストに含める

**実装コード**:
```typescript
// 1. 重要文を抽出
const extractedSentences = extractImportantSentences(
  relevantDocs.map(d => d.content).join('\n'),
  { maxSentences: 10 }
);

// 2. 抽出文と生成型要約を組み合わせ
const context = [
  ...extractedSentences.map(s => s.text),
  ...relevantDocs.map(d => d.content)
].join('\n\n');
```

**テスト**: `src/ai/flows/__tests__/hybrid-summarization.test.ts` (新規作成)

---

### Step 6.2: テストと検証（1週間）

**ファイル**: `scripts/test-hybrid-summarization-quality.ts` (新規作成)

**テスト内容**:
1. 生成型のみとハイブリッド要約を比較
2. 要約品質を評価（ROUGEスコア等）
3. ユーザーテスト

**検証項目**:
- [ ] 要約品質が向上すること
- [ ] 重要な情報が欠落しないこと
- [ ] 処理時間が許容範囲内であること

---

**Phase 6 完了条件**:
- [ ] ハイブリッド要約が実装されていること
- [ ] 要約品質が向上していること
- [ ] 統合テストがパスすること

---

## Phase 7: 要約粒度の調整機能（1週間）

### 🎯 目標
- ユーザーが要約の長さを調整可能
- セクション単位での要約表示

---

### Step 7.1: UI実装（3-4日）

#### 実装タスク

**7.1.1: 要約粒度選択UI**

**ファイル**: `src/components/summary-granularity-selector.tsx` (新規作成)

**実装内容**:
- ラジオボタンまたはドロップダウンで粒度を選択
- オプション: 短い / 標準 / 詳細

**UI例**:
```
要約の詳細度: [ 短い ] [ 標準 ] [ 詳細 ]
```

---

**7.1.2: API拡張**

**ファイル**: `src/app/api/streaming-process/route.ts`

**変更内容**:
1. リクエストに`summaryGranularity`パラメータを追加
2. プロンプトを粒度に応じて調整

**実装コード**:
```typescript
interface StreamingProcessRequest {
  query: string;
  source?: 'confluence' | 'jira';
  summaryGranularity?: 'short' | 'standard' | 'detailed';
}
```

---

### Step 7.2: プロンプト調整（2-3日）

**ファイル**: `src/ai/flows/streaming-summarize-confluence-docs.ts`

**変更内容**:
1. 粒度に応じてプロンプトを調整
2. トークン数を制限

**実装コード**:
```typescript
const PROMPT_TEMPLATES = {
  short: '簡潔に3-5文で要約してください...',
  standard: '標準的な長さで要約してください...',
  detailed: '詳細な要約を提供してください...'
};
```

---

### Step 7.3: テスト（1-2日）

**ファイル**: `src/tests/feature/summary-granularity.test.ts` (新規作成)

**検証項目**:
- [ ] 各粒度で適切な長さの要約が生成されること
- [ ] UIで粒度を切り替えられること

---

**Phase 7 完了条件**:
- [ ] UIが実装されていること
- [ ] 各粒度で要約が生成されること
- [ ] テストがパスすること

---

## 📊 全体スケジュール

| Phase | 期間 | 開始週 | 完了週 |
|-------|------|--------|--------|
| Phase 1: チャンキング改善 | 3-4週間 | Week 1 | Week 4 |
| Phase 2: チャンクサイズ統一 | 1週間 | Week 4 | Week 5 |
| Phase 3: メタデータ強化 | 2-3週間 | Week 5 | Week 8 |
| Phase 4: セクション表示 | 1週間 | Week 8 | Week 9 |
| Phase 5: OCR対応 | 3-4週間 | Week 9 | Week 13 |
| Phase 6: ハイブリッド要約 | 2週間 | Week 13 | Week 15 |
| Phase 7: 粒度調整 | 1週間 | Week 15 | Week 16 |

**総期間**: 約16週間（4ヶ月）

---

## ✅ 各Phaseの完了条件チェックリスト

### Phase 1: チャンキング改善
- [ ] セマンティックチャンキング関数が実装されている
- [ ] 全データソースで統合されている
- [ ] ユニットテストが全てパス
- [ ] 統合テストが全てパス
- [ ] パフォーマンステストが許容範囲内
- [ ] 検索品質テストで改善を確認
- [ ] コードレビュー完了

### Phase 2: チャンクサイズ統一
- [ ] Google Driveのチャンクサイズが1800文字
- [ ] 全データソースで一貫性を確認
- [ ] 仕様書を更新

### Phase 3: メタデータ強化
- [ ] メタデータスキーマが拡張されている
- [ ] Markdown解析機能が実装されている
- [ ] 全データソースでメタデータが設定される
- [ ] LanceDBスキーマが更新されている
- [ ] 統合テストがパス

### Phase 4: セクション表示
- [ ] 検索結果にセクション情報が表示される
- [ ] UIが実装されている
- [ ] UIテストがパス

### Phase 5: OCR対応
- [ ] OCR機能が実装されている
- [ ] PDF内画像からテキストが抽出される
- [ ] 画像ファイルからテキストが抽出される
- [ ] 精度が許容範囲内

### Phase 6: ハイブリッド要約
- [ ] 抽出型要約が実装されている
- [ ] 生成型要約と統合されている
- [ ] 要約品質が向上している
- [ ] 統合テストがパス

### Phase 7: 粒度調整
- [ ] UIが実装されている
- [ ] APIが拡張されている
- [ ] 各粒度で要約が生成される
- [ ] テストがパス

---

## 🧪 テスト戦略

### 1. ユニットテスト
- 各関数・メソッドの動作を個別にテスト
- カバレッジ: 80%以上

### 2. 統合テスト
- 複数コンポーネントの連携をテスト
- 既存機能への影響を確認

### 3. パフォーマンステスト
- 処理時間、メモリ使用量を測定
- 既存実装との比較

### 4. 品質テスト
- 検索精度、要約品質を評価
- ユーザーテストケースでの検証

### 5. 回帰テスト
- 既存機能が正常に動作することを確認
- 既存のテストスイートを実行

---

## 📝 進捗管理

### 週次レビュー
- 毎週金曜日に進捗をレビュー
- ブロッカーやリスクを早期発見
- スケジュール調整

### デイリースタンドアップ
- 毎日の進捗確認（オプション）
- ブロッカーの共有

### 完了報告
- 各Phase完了時に報告書を作成
- テスト結果を記録
- 次のPhaseへの引き継ぎ

---

## 🔄 ロールバック計画

各Phaseで問題が発生した場合のロールバック手順:

1. **Phase 1-2**: 既存の`chunkText`関数に戻す（簡単）
2. **Phase 3**: 新しいメタデータフィールドを無視（オプショナルなので安全）
3. **Phase 4-7**: 機能フラグで無効化可能にする

---

## 📚 参考資料

- [セマンティックチャンキング実装例](./semantic-chunking-example.md)
- [メタデータ拡張仕様](./metadata-extension-spec.md)
- [テスト計画](./test-plan.md)

